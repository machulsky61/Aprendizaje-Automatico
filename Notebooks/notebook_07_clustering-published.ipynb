{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_07_clustering-published.ipynb)\n","\n","# Clustering\n","\n","En este notebook implementaremos algunos de los conceptos de _clustering_ vistos en clase.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from scipy.cluster.hierarchy import dendrogram\n","from matplotlib import pyplot as plt\n","\n","plt.rcParams['figure.figsize'] = [10, 5] # para ver los gráficos más grandes"]},{"cell_type":"markdown","metadata":{},"source":["## Probando K-means y DBSCAN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%matplotlib inline \n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import cluster, datasets\n","from sklearn.preprocessing import StandardScaler\n","\n","# crear y normalizar dataset\n","N = 1500\n","X, y = datasets.make_blobs(n_samples=N, centers=3, cluster_std=1.0,random_state=2211)\n","X = StandardScaler().fit_transform(X)\n","\n","# ejecutar k-means con k=2\n","algorithm = cluster.MiniBatchKMeans(n_clusters=2)\n","algorithm.fit(X)\n","y_pred = algorithm.labels_.astype(int)\n","\n","# graficar\n","colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n","colors = np.hstack([colors] * 20)\n","plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Ejercicio** \n","1. Encontrar clusters sobre el mismo dataset utilizando la implementacion de sklearn de [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).\n","2. Experimentar con los hiperparámetros de ambos modelos."]},{"cell_type":"markdown","metadata":{},"source":["# Implementaciones de algoritmos\n","\n","En esta sección implementarán K-Means, Clustering Aglomerativo y GMMs."]},{"cell_type":"markdown","metadata":{},"source":["## K-Means\n","\n","Recordemos el algoritmo de KMeans:"]},{"cell_type":"markdown","metadata":{},"source":["```\n","Hiperparámetros: K\n","\n","1. Ubicar K centroides al azar.\n","2. Iterar hasta cumplir con algún criterio de convergencia:\n","\n","   a. Computar distancias (euclidianas) entre cada punto a cada uno de los centroides.\n","\n","   b. A cada punto, asignar cluster según cercanía a cada centroide.\n","   \n","   c. Mover centroides a la media de cada cluster.\n","\n","El centroide de un cluster es el vector promedio de las instancias pertenecientes al cluster (vector con promedios de cada atributo).\n","```"]},{"cell_type":"markdown","metadata":{},"source":["1. Definir la función que inicializa los k centroides al azar: `inicializacion_centroides`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def inicializacion_centroides(puntos: np.ndarray, k: int) -> np.ndarray:\n","    \"\"\"Inicializa los k centroides eligiendo de manera random k de\n","      los puntos\"\"\"\n","    #COMPLETARS\n","    return centroides\n"]},{"cell_type":"markdown","metadata":{},"source":["2. Completar la función `centroides_cercanos` que calcula, para cada punto, el centroide más cercano que será además donde tendremos a qué cluster pertenece cada uno de los puntos."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def centroides_cercanos(puntos: np.ndarray, centroides: np.ndarray) -> np.ndarray:\n","    \"\"\" Devuelve un array que contiene el indice al centroide más cercano para cada punto.\"\"\"\n","    #COMPLETAR\n","    return distancias"]},{"cell_type":"markdown","metadata":{},"source":["3. Completar la función `mover_centroides` que mueve a los centroides al punto medio de cada cluster y devuelve los nuevos centroides."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def mover_centroides(puntos: np.ndarray, cercanos: np.ndarray, k: int) -> np.ndarray:\n","    \"\"\" Mueve los centroides a la media de cada cluster.\"\"\"\n","    #COMPLETAR\n","    return nuevos_centroides"]},{"cell_type":"markdown","metadata":{},"source":["4. Probar la implementación de las funciones"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def kmeans(puntos: np.ndarray, k: int, max_iters: int = 100) -> np.ndarray:\n","    \"\"\" Algoritmo K-means. \"\"\"\n","    centroides = inicializacion_centroides(puntos, k)\n","    for _ in range(max_iters):\n","        nuevos_clusters = centroides_cercanos(puntos, centroides)\n","        centroides = mover_centroides(puntos, nuevos_clusters, k)\n","        \n","    return centroides, nuevos_clusters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.random.seed(42)\n","puntos = np.random.rand(100, 2)  #100 puntos en dimension 2\n","k = 3  \n","centroids, closest = kmeans(puntos, k)\n","\n","plt.scatter(puntos[:, 0], puntos[:, 1], c=closest, s=50, cmap='viridis')\n","plt.scatter(centroids[:, 0], centroids[:, 1], color='red', s=200, alpha=0.5)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Clustering aglomerativo\n"]},{"cell_type":"markdown","metadata":{},"source":["Ahora compleatremos un algoritmo de clustering jerárquico que utiliza una estrategia _bottom-up_.\n","\n","### Pseudo-código\n","\n","```\n","1. Cada punto es un cluster: C1 = x(1); … ; Cn = x(n) \n","2. Calcular una matriz de distancias entre todo par de clusters. \n","3. Buscar en la matriz de distancia el par de clusters más similares. \n","4. Mergear 2 clusters: A y B según su distancia. \n","    a. Borrar las filas de la matriz de distancias correspondientes a los clusters A y B. \n","    b. Agregar una nueva fila que contenga la distancia entre A ∪ B y el resto de los clusters. \n","5. Repetir los pasos 3 y 4 hasta conseguir la cantidad de clusters deseados (o hasta que haya sólo un cluster). \n","```\n","\n","El objetivo será tener una función:\n","\n","```python\n","def aglomerativo(datos, num_clusters=1, criterio_distancia=\"min\")\n","```\n","\n","Para ello se propone:\n","\n","1. Implementar la función `inicializar_distancias(datos)` que toma una matriz de numpy, donde cada fila es una instancia y las columnas son los features y devuelve una matriz de distancias. Las matriz debe ser triangular inferior para calcular las distancias, los valores en y sobre la diagonal deben ser `NaN`.\n","En este caso usaremos la distancia Euclediana."]},{"cell_type":"raw","metadata":{"vscode":{"languageId":"raw"}},"source":["def inicializar_distancias(datos: np.ndarray) -> np.ndarray:\n","    \n","    #COMPLETAR"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tests:\n","X_0 = np.array([[1], [2], [5], [-3]])\n","y_0_resp = inicializar_distancias(X_0)\n","y_0_exp = np.array([[np.nan, np.nan, np.nan, np.nan], [1., np.nan, np.nan, np.nan], [ 4., 3., np.nan, np.nan], [4.,  5., 8., np.nan]])\n","np.testing.assert_allclose(y_0_resp, y_0_exp, rtol=1e-5)\n","print(\"Pasé test 0\")\n","\n","\n","X_1 = np.array([[1,2], [2,3], [3,5]])\n","y_1_resp = inicializar_distancias(X_1)\n","y_1_exp = np.array([[np.nan, np.nan, np.nan], [1.41421356, np.nan, np.nan], [3.60555128, 2.23606798, np.nan]])\n","np.testing.assert_allclose(y_1_resp, y_1_exp, rtol=1e-5)\n","print(\"Pasé test 1\")\n","\n","\n","X_2 = np.array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2]])\n","y_2_resp = inicializar_distancias(X_2)\n","y_2_exp = np.array([[np.nan, np.nan, np.nan, np.nan, np.nan],  [0.53851648, np.nan, np.nan, np.nan, np.nan],  [0.50990195, 0.3, np.nan, np.nan, np.nan],  [0.64807407, 0.33166248, 0.24494897, np.nan, np.nan],  [0.14142136, 0.60827625, 0.50990195, 0.64807407, np.nan]])\n","np.testing.assert_allclose(y_2_resp, y_2_exp, rtol=1e-5)\n","print(\"Pasé test 2\")"]},{"cell_type":"markdown","metadata":{},"source":["2. Definir la función `encontrar minimo(distancias)` que toma una matriz de distancias como la que se generó en el punto anterior y devuelve la fila y la columna de su menor valor."]},{"cell_type":"raw","metadata":{"vscode":{"languageId":"raw"}},"source":["from typing import Tuple\n","\n","def encontrar_minimo(distancias: np.ndarray) -> Tuple[int, int]:\n","    \n","    #COMPLETAR\n","    \n","    return (2, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tests:\n","\n","d_0 = np.array([[np.nan, np.nan, np.nan, np.nan], [1., np.nan, np.nan, np.nan], [ 4., 3., np.nan, np.nan], [4.,  5., 8., np.nan]])\n","pos_0_resp = encontrar_minimo(d_0)\n","pos_0_exp = (1,0)\n","np.testing.assert_allclose(pos_0_resp, pos_0_exp, rtol=1e-5)\n","print(\"Pasé test 0\")\n","\n","\n","d_1 = np.array([[np.nan, np.nan, np.nan], [1.41421356, np.nan, np.nan], [3.60555128, 2.23606798, np.nan]])\n","pos_1_resp = encontrar_minimo(d_1)\n","pos_1_exp = (1,0)\n","np.testing.assert_allclose(pos_1_resp, pos_1_exp, rtol=1e-5)\n","print(\"Pasé test 1\")\n","\n","\n","d_2 = np.array([[np.nan, np.nan, np.nan, np.nan, np.nan],  [0.53851648, np.nan, np.nan, np.nan, np.nan],  [0.50990195, 0.3, np.nan, np.nan, np.nan],  [0.64807407, 0.33166248, 0.24494897, np.nan, np.nan],  [0.14142136, 0.60827625, 0.50990195, 0.64807407, np.nan]])\n","pos_2_resp = encontrar_minimo(d_2)\n","pos_2_exp = (4,0)\n","np.testing.assert_allclose(pos_2_resp, pos_2_exp, rtol=1e-5)\n","print(\"Pasé test 2\")"]},{"cell_type":"markdown","metadata":{},"source":["3. Definir la función `actualizar_distancias(distancias, i, j, criterio=\"min\")` que toma una matriz de distancias y recalcula la distancia de un nuevo cluster que sea la unión del que tiene índice $i$ con el de índice $j$, que saque las columnas $i$ y $j$ y agregue una nueva columna/fila al final con las distancias de este nuevo cluster armado con los demás. Debe al menos tener el criterio de distancias entre clusters \"mínimo\" u \"máximo\", opcionalmente se puede completar con \"vinculación promedio\"."]},{"cell_type":"raw","metadata":{"vscode":{"languageId":"raw"}},"source":["def actualizar_distancias(d: np.ndarray, i: int, j: int, criterio: str = \"min\") -> np.ndarray:\n","    \n","    #COMPLETAR\n","    \n","    return distancias"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tests:\n","\n","d_0 = np.array([[np.nan, np.nan, np.nan, np.nan], [1., np.nan, np.nan, np.nan], [ 4., 3., np.nan, np.nan], [4.,  5., 8., np.nan]])\n","pos_0_resp = actualizar_distancias(d_1,1,0)\n","pos_0_exp = np.array([[np.nan, np.nan], [2.23606798, np.nan]])\n","np.testing.assert_allclose(pos_0_resp, pos_0_exp, rtol=1e-5)\n","print(\"Pasé test 0\")\n"]},{"cell_type":"markdown","metadata":{},"source":["4. Revisar el código de `aglomerativo` y correrlo para el ejemplo\n","\n","    ```python\n","        X = np.array([[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]])\n","\n","        cl, u = aglomerativo(X, criterio=\"max\")\n","\n","        fig = plt.figure(figsize=(10, 6),facecolor='white')\n","        dn = dendrogram(u)\n","    ```\n","\n","    Verificar que obtienen un resultado equivalente al mostrado como ejemplo de [esta página](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def aglomerativo(datos, num_clusters=1, criterio_distancia=\"min\"):\n","    distancias = inicializar_distancias(datos)\n","\n","    uniones = []\n","    clusters = [[idx] for idx,_ in enumerate(datos)]\n","    nombres_de_clusters = [idx for idx,_ in enumerate(datos)]\n","    prox_cluster = len(nombres_de_clusters) \n","\n","    while distancias.shape[0] > num_clusters:\n","        # Buscar el par de clusters más similares\n","        i, j = encontrar_minimo(distancias)\n","        d = distancias[i][j]\n","        \n","        # Fusionar los clusters A y B\n","        nuevo_cluster = clusters[i]\n","        nuevo_cluster.extend(clusters[j])\n","        clusters.append(nuevo_cluster)\n","\n","        # agrego la unión que realizo\n","        uniones.append([nombres_de_clusters[i], nombres_de_clusters[j], d, len(nuevo_cluster)])\n","\n","        # agrego la composición del nuevo cluster\n","        nombres_de_clusters.append(prox_cluster)\n","        prox_cluster+=1\n","\n","        # Actualizar la matriz de distancias\n","        distancias = actualizar_distancias(distancias, i, j, criterio=criterio_distancia)\n","\n","        # Sacar la info de los clusters sin fusionar\n","        clusters = [c for idx,c in enumerate(clusters) if idx not in [i,j]]\n","        nombres_de_clusters = [n for idx,n in enumerate(nombres_de_clusters) if idx not in [i,j]]\n","\n","\n","    return clusters, np.array(uniones)\n"]},{"cell_type":"markdown","metadata":{},"source":["5. Crear un dendograma con el dataset de Iris y verificar si la clusterización generada es o no consistente con lo expresado en los targets\n","\n","    ```python\n","        # Cargamos el dataset que usaremos hoy\n","        from sklearn.datasets import load_iris\n","        iris_dataset = load_iris()\n","\n","        X = pd.DataFrame(iris_dataset.data)\n","        y = pd.Series(iris_dataset.target)\n","    ```\n","\n","    Explorar las siguientes condiciones:\n","\n","    1. Criterio de distancias\n","\n","    1. ¿Se obtiene una mejor, peor o similar clusterización al normalizar los atributos?\n"," "]},{"cell_type":"markdown","metadata":{},"source":["##  EM para GMMs"]},{"cell_type":"markdown","metadata":{},"source":["### Inicialización:\n","Decidir cuantos clusters (c) utilizaremos. Luego, inicializar los parámetros para $c$ normales: $c$ medias ($\\mu_c$), $c$  desvios ($\\sigma_c$), y $c$ pesos ($\\pi_c$).\n","\n","### Paso E:\n","\n","Calcular para cada punto $x^{(i)}$ la probabilidad ($r_{ic}$) de que el punto pertenezca al cluster c:\n","$$r_{ic} = P(c | x^{(i)}) = \\frac{P(c)P(x^{(i)} | c)}{P(x^{(i)})} = \\frac{\\pi_c N(x^{(i)} \\ | \\ \\mu_c,\\sigma_c)}{\\Sigma_{k=1}^K \\pi_k N(x^{(i)} \\ | \\ \\mu_k,\\sigma_k)}$$\n","\n","donde \n","\n","$${\\displaystyle {\\begin{aligned}N(x^{(i)},\\mu_c,\\sigma_c)&{}={\\frac {1}{\\sigma_c {\\sqrt {2\\pi }}}}e^{-{\\frac {(x^{(i)}-\\mu_c )^{2}}{2\\sigma_c ^{2}}}},\\quad \\pi = 3.141592... .\\\\\\end{aligned}}}$$\n","\n","### Paso M:\n","Para cada cluster c, actualizar $\\pi_c$, $\\mu_c$, y $\\sigma_c$ según los datos:\n","\n","$$ N_c = \\sum_{i}{r_{ic}}$$\n","\n","$$ \\mu^{(nuevo)}_c = \\frac{1}{N_c}\\sum_{i}{r_{ic}x^{(i)}}$$\n","$$ \\sigma^{(nuevo)}_c = \\frac{1}{N_c}\\sum_{i}{r_{ic}(x^{(i)} - \\mu^{(nuevo)}_c)^2} $$\n","$$ \\pi^{(nuevo)}_c = \\frac{N_c}{n}$$\n","\n","\n","\n","### Iterar \n","Iterar hasta que la log-likelihood del modelo converja:\n","\n","$$ln \\ p(\\boldsymbol{X} \\ | \\ \\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\ = \\ \\Sigma_{i=1}^N \\ ln(\\Sigma_{k=1}^K \\pi_k N(x^{(i)} \\ | \\ \\mu_k,\\sigma_k))$$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import scipy.stats\n","from sklearn.cluster import KMeans\n","import math \n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.mixture import GaussianMixture\n","\n","def gmm_train(X, n_clusters, tolerance = 1e-3):\n","    v_mu, v_sigma, v_pi = inicializacion(n_clusters, X)\n","    old_llk = 0\n","    tolerance = 1e-3 # indica la mínima ganancia en llk para que el algoritmo continúe.\n","    gain = np.inf\n","\n","    while(gain > tolerance):\n","        print_iter(v_mu, v_sigma, v_pi, n_clusters)\n","        r = e(X, n_clusters, v_mu, v_sigma, v_pi)\n","        v_mu, v_sigma, v_pi = m(X, n_clusters, r)\n","        llk = log_likelihood(X, v_mu, v_sigma, v_pi, n_clusters)\n","        print(\"Log likelihood: \", llk)\n","        gain = llk - old_llk\n","   \n","    return v_mu, v_sigma, v_pi\n","\n","def inicializacion(n_clusters, X):\n","    ##################\n","    # COMPLETAR\n","    # Inicializar v_mu, v_sigma, v_pi (vectores que contienen una componente por cluster)\n","    ##################\n","    \n","    return v_mu, v_sigma, v_pi\n","\n","def norm(x, mu, sigma):\n","    return scipy.stats.norm(mu, sigma).pdf(x)\n","\n","def e(X, n_clusters, v_mu, v_sigma, v_pi):\n","    r = np.ones((len(X), n_clusters))\n","    \n","    ##################\n","    # COMPLETAR\n","    # r[i, c] debera contener la probabilidad estimada P(c | x^(i))\n","    ##################\n","    \n","    return r\n","\n","def m(X, n_clusters, r):\n","    N = np.zeros(n_clusters)\n","    v_mu_new = np.zeros(n_clusters)\n","    v_sigma_new = np.zeros(n_clusters)\n","    v_pi_new = np.zeros(n_clusters)\n","    \n","    ##################\n","    # COMPLETAR\n","    # v_mu_new, v_sigma_new, v_pi_new deberán contener los valores actualizados.\n","    ##################\n","    \n","    return v_mu_new, v_sigma_new, v_pi_new\n","\n","\n","def log_likelihood(X, v_mu, v_sigma, v_pi, n_clusters):\n","    suma = 0\n","    for i in range(len(X)):\n","        suma_k = 0\n","        for k in range(n_clusters):\n","            suma_k += v_pi[k] * norm(X[i], v_mu[k], v_sigma[k])\n","        suma += math.log(suma_k)\n","    return suma\n","\n","\n","def print_iter( v_mu, v_sigma, v_pi, n_clusters):\n","    for c in range(n_clusters):\n","        print(\"C{} => mu={} +- {} (pi={})\".format(c+1, \"%.2f\" % v_mu[c], \"%.2f\" % v_sigma[c], \"%.2f\" % v_pi[c]), end=\" | \")\n","    print()\n","\n","def dibujar(X, v_mu, v_sigma, v_pi, n_clusters, title=\"GMMs\"):\n","    plt.figure()\n","    plt.title(title)\n","    colors = sns.color_palette(\"hls\", n_clusters)\n","    plt.plot(X, [1] * len(X), \".\")\n","    for k in range(n_clusters):\n","        col = colors[k]\n","#         plt.plot([v_mu[k], v_mu[k]], [0, 2], label=\"$\\mu_{}$\".format(k), color=col)\n","        plt.plot([v_mu[k] - v_sigma[k], v_mu[k] + v_sigma[k]], [1.1,1.1], color=col, label=\"$\\mu_{}$\".format(k))\n","        plt.plot([v_mu[k] - v_sigma[k], v_mu[k] - v_sigma[k]], [0.9,1.2], color=col)\n","        plt.plot([v_mu[k] + v_sigma[k], v_mu[k] + v_sigma[k]], [0.9,1.2], color=col)\n","    plt.yticks([])\n","    plt.ylim([0, 2])\n","    plt.legend()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Datos para ejemplo\n","np.random.seed(134)\n","X = np.array([10, 11, 12, 20, 21, 22, 30, 31, 32, 29, 33])\n","n_clusters = 3\n","\n","# DESCOMENTAR\n","v_mu, v_sigma, v_pi = gmm_train(X, n_clusters)\n","dibujar(X, v_mu, v_sigma, v_pi, n_clusters, title=\"implementacion propia\")\n","#print(\"DESCOMENTAR lo anterior\")\n","\n","print(\"Salida esperada:\")\n","# SALIDA ESPERADA:\n","sklearn_gmm = GaussianMixture(n_components=3)\n","sklearn_gmm.fit(X.reshape(-1, 1))\n","\n","dibujar(X, sklearn_gmm.means_.ravel(), sklearn_gmm.covariances_.ravel(), sklearn_gmm.weights_.ravel(), n_clusters, title=\"sklearn\")\n"]},{"cell_type":"markdown","metadata":{},"source":["-------------------------------------------------------------------"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
