{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_09_redes_neuronales-published.ipynb)\n", "\n", "# Redes neuronales\n", "\n", "\n", "Vamos nuevamente a trabajar con los datos de `iris` para entrenar (y antes construir) una Red Neuronal."]}, {"cell_type": "code", "execution_count": 387, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[5.1, 3.5, 1.4, 0.2],\n", "       [4.9, 3. , 1.4, 0.2],\n", "       [4.7, 3.2, 1.3, 0.2],\n", "       [4.6, 3.1, 1.5, 0.2],\n", "       [5. , 3.6, 1.4, 0.2],\n", "       [5.4, 3.9, 1.7, 0.4],\n", "       [4.6, 3.4, 1.4, 0.3],\n", "       [5. , 3.4, 1.5, 0.2],\n", "       [4.4, 2.9, 1.4, 0.2],\n", "       [4.9, 3.1, 1.5, 0.1],\n", "       [5.4, 3.7, 1.5, 0.2],\n", "       [4.8, 3.4, 1.6, 0.2],\n", "       [4.8, 3. , 1.4, 0.1],\n", "       [4.3, 3. , 1.1, 0.1],\n", "       [5.8, 4. , 1.2, 0.2],\n", "       [5.7, 4.4, 1.5, 0.4],\n", "       [5.4, 3.9, 1.3, 0.4],\n", "       [5.1, 3.5, 1.4, 0.3],\n", "       [5.7, 3.8, 1.7, 0.3],\n", "       [5.1, 3.8, 1.5, 0.3],\n", "       [5.4, 3.4, 1.7, 0.2],\n", "       [5.1, 3.7, 1.5, 0.4],\n", "       [4.6, 3.6, 1. , 0.2],\n", "       [5.1, 3.3, 1.7, 0.5],\n", "       [4.8, 3.4, 1.9, 0.2],\n", "       [5. , 3. , 1.6, 0.2],\n", "       [5. , 3.4, 1.6, 0.4],\n", "       [5.2, 3.5, 1.5, 0.2],\n", "       [5.2, 3.4, 1.4, 0.2],\n", "       [4.7, 3.2, 1.6, 0.2],\n", "       [4.8, 3.1, 1.6, 0.2],\n", "       [5.4, 3.4, 1.5, 0.4],\n", "       [5.2, 4.1, 1.5, 0.1],\n", "       [5.5, 4.2, 1.4, 0.2],\n", "       [4.9, 3.1, 1.5, 0.2],\n", "       [5. , 3.2, 1.2, 0.2],\n", "       [5.5, 3.5, 1.3, 0.2],\n", "       [4.9, 3.6, 1.4, 0.1],\n", "       [4.4, 3. , 1.3, 0.2],\n", "       [5.1, 3.4, 1.5, 0.2],\n", "       [5. , 3.5, 1.3, 0.3],\n", "       [4.5, 2.3, 1.3, 0.3],\n", "       [4.4, 3.2, 1.3, 0.2],\n", "       [5. , 3.5, 1.6, 0.6],\n", "       [5.1, 3.8, 1.9, 0.4],\n", "       [4.8, 3. , 1.4, 0.3],\n", "       [5.1, 3.8, 1.6, 0.2],\n", "       [4.6, 3.2, 1.4, 0.2],\n", "       [5.3, 3.7, 1.5, 0.2],\n", "       [5. , 3.3, 1.4, 0.2],\n", "       [7. , 3.2, 4.7, 1.4],\n", "       [6.4, 3.2, 4.5, 1.5],\n", "       [6.9, 3.1, 4.9, 1.5],\n", "       [5.5, 2.3, 4. , 1.3],\n", "       [6.5, 2.8, 4.6, 1.5],\n", "       [5.7, 2.8, 4.5, 1.3],\n", "       [6.3, 3.3, 4.7, 1.6],\n", "       [4.9, 2.4, 3.3, 1. ],\n", "       [6.6, 2.9, 4.6, 1.3],\n", "       [5.2, 2.7, 3.9, 1.4],\n", "       [5. , 2. , 3.5, 1. ],\n", "       [5.9, 3. , 4.2, 1.5],\n", "       [6. , 2.2, 4. , 1. ],\n", "       [6.1, 2.9, 4.7, 1.4],\n", "       [5.6, 2.9, 3.6, 1.3],\n", "       [6.7, 3.1, 4.4, 1.4],\n", "       [5.6, 3. , 4.5, 1.5],\n", "       [5.8, 2.7, 4.1, 1. ],\n", "       [6.2, 2.2, 4.5, 1.5],\n", "       [5.6, 2.5, 3.9, 1.1],\n", "       [5.9, 3.2, 4.8, 1.8],\n", "       [6.1, 2.8, 4. , 1.3],\n", "       [6.3, 2.5, 4.9, 1.5],\n", "       [6.1, 2.8, 4.7, 1.2],\n", "       [6.4, 2.9, 4.3, 1.3],\n", "       [6.6, 3. , 4.4, 1.4],\n", "       [6.8, 2.8, 4.8, 1.4],\n", "       [6.7, 3. , 5. , 1.7],\n", "       [6. , 2.9, 4.5, 1.5],\n", "       [5.7, 2.6, 3.5, 1. ],\n", "       [5.5, 2.4, 3.8, 1.1],\n", "       [5.5, 2.4, 3.7, 1. ],\n", "       [5.8, 2.7, 3.9, 1.2],\n", "       [6. , 2.7, 5.1, 1.6],\n", "       [5.4, 3. , 4.5, 1.5],\n", "       [6. , 3.4, 4.5, 1.6],\n", "       [6.7, 3.1, 4.7, 1.5],\n", "       [6.3, 2.3, 4.4, 1.3],\n", "       [5.6, 3. , 4.1, 1.3],\n", "       [5.5, 2.5, 4. , 1.3],\n", "       [5.5, 2.6, 4.4, 1.2],\n", "       [6.1, 3. , 4.6, 1.4],\n", "       [5.8, 2.6, 4. , 1.2],\n", "       [5. , 2.3, 3.3, 1. ],\n", "       [5.6, 2.7, 4.2, 1.3],\n", "       [5.7, 3. , 4.2, 1.2],\n", "       [5.7, 2.9, 4.2, 1.3],\n", "       [6.2, 2.9, 4.3, 1.3],\n", "       [5.1, 2.5, 3. , 1.1],\n", "       [5.7, 2.8, 4.1, 1.3],\n", "       [6.3, 3.3, 6. , 2.5],\n", "       [5.8, 2.7, 5.1, 1.9],\n", "       [7.1, 3. , 5.9, 2.1],\n", "       [6.3, 2.9, 5.6, 1.8],\n", "       [6.5, 3. , 5.8, 2.2],\n", "       [7.6, 3. , 6.6, 2.1],\n", "       [4.9, 2.5, 4.5, 1.7],\n", "       [7.3, 2.9, 6.3, 1.8],\n", "       [6.7, 2.5, 5.8, 1.8],\n", "       [7.2, 3.6, 6.1, 2.5],\n", "       [6.5, 3.2, 5.1, 2. ],\n", "       [6.4, 2.7, 5.3, 1.9],\n", "       [6.8, 3. , 5.5, 2.1],\n", "       [5.7, 2.5, 5. , 2. ],\n", "       [5.8, 2.8, 5.1, 2.4],\n", "       [6.4, 3.2, 5.3, 2.3],\n", "       [6.5, 3. , 5.5, 1.8],\n", "       [7.7, 3.8, 6.7, 2.2],\n", "       [7.7, 2.6, 6.9, 2.3],\n", "       [6. , 2.2, 5. , 1.5],\n", "       [6.9, 3.2, 5.7, 2.3],\n", "       [5.6, 2.8, 4.9, 2. ],\n", "       [7.7, 2.8, 6.7, 2. ],\n", "       [6.3, 2.7, 4.9, 1.8],\n", "       [6.7, 3.3, 5.7, 2.1],\n", "       [7.2, 3.2, 6. , 1.8],\n", "       [6.2, 2.8, 4.8, 1.8],\n", "       [6.1, 3. , 4.9, 1.8],\n", "       [6.4, 2.8, 5.6, 2.1],\n", "       [7.2, 3. , 5.8, 1.6],\n", "       [7.4, 2.8, 6.1, 1.9],\n", "       [7.9, 3.8, 6.4, 2. ],\n", "       [6.4, 2.8, 5.6, 2.2],\n", "       [6.3, 2.8, 5.1, 1.5],\n", "       [6.1, 2.6, 5.6, 1.4],\n", "       [7.7, 3. , 6.1, 2.3],\n", "       [6.3, 3.4, 5.6, 2.4],\n", "       [6.4, 3.1, 5.5, 1.8],\n", "       [6. , 3. , 4.8, 1.8],\n", "       [6.9, 3.1, 5.4, 2.1],\n", "       [6.7, 3.1, 5.6, 2.4],\n", "       [6.9, 3.1, 5.1, 2.3],\n", "       [5.8, 2.7, 5.1, 1.9],\n", "       [6.8, 3.2, 5.9, 2.3],\n", "       [6.7, 3.3, 5.7, 2.5],\n", "       [6.7, 3. , 5.2, 2.3],\n", "       [6.3, 2.5, 5. , 1.9],\n", "       [6.5, 3. , 5.2, 2. ],\n", "       [6.2, 3.4, 5.4, 2.3],\n", "       [5.9, 3. , 5.1, 1.8]])"]}, "execution_count": 387, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn.preprocessing import LabelEncoder\n", "from sklearn.datasets import load_iris\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def get_data():\n", "    dataset = load_iris()\n", "    X = dataset[\"data\"]\n", "    y = dataset[\"target\"]\n", "    y = LabelEncoder().fit_transform(y)\n", "    return np.array(X), np.array(y)\n", "X, y = get_data()\n", "X"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La propuesta es empezar por el esqueleto de las 2 clases que usaremos para esta tarea e ir implementado los m\u00e9todos a medida que avancemos.\n", "\n", "Al final de este notebook se encuentran ambas clases completas. Pueden copiar el c\u00f3digo desde all\u00ed mismo o implementarlo. La idea es que en cada avance podamos comprender la parte del proceso que estamos realizando, por lo cual se recomienda seguir la guia propuesta e ir completando s\u00f3lo lo que es necesario para cada punto."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["class Capa:\n", "    def __init__(self, neuronas):\n", "        self.neuronas = neuronas\n", "\n", "    def forward(self, inputs, weights, bias, activation):\n", "        \"\"\"\n", "        Forward Propagation de la capa\n", "        \"\"\"\n", "        raise NotImplementedError\n", "        \n", "    def relu(self, inputs):\n", "        \"\"\"\n", "        ReLU: funci\u00f3n de activaci\u00f3n\n", "        \"\"\"\n", "        raise NotImplementedError\n", "\n", "    def softmax(self, inputs):\n", "        \"\"\"\n", "        Softmax: funci\u00f3n de activaci\u00f3n\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n", "        \"\"\"\n", "        Backward Propagation de la capa\n", "        \"\"\"\n", "        raise NotImplementedError\n", "\n", "    def relu_derivative(self, dA, Z):\n", "        \"\"\"\n", "        ReLU: gradiente de ReLU\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    "]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["class RedNeuronal:\n", "    def __init__(self, learning_rate=0.01):\n", "        self.red = [] ## capas\n", "        self.arquitectura = [] ## mapeo de entradas -> salidas\n", "        self.pesos = [] ## W, b\n", "        self.memoria = [] ## Z, A\n", "        self.gradientes = [] ## dW, db\n", "        self.lr = learning_rate\n", "        \n", "    def add(self, capa):\n", "        \"\"\"\n", "        Agregar capa a la red\n", "        \"\"\"\n", "        self.network.append(capa)\n", "            \n", "    def _compile(self, data):\n", "        \"\"\"\n", "        Inicializar la arquitectura\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _init_weights(self, data):\n", "        \"\"\"\n", "        Inicializar arquitectura y los pesos\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _forwardprop(self, data):\n", "        \"\"\"\n", "        Pasada forward completa por la red\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _backprop(self, predicted, actual):\n", "        \"\"\"\n", "        Pasada backward completa por la red\n", "        \"\"\"\n", "        raise NotImplementedError\n", "            \n", "    def _update(self):\n", "        \"\"\"\n", "        Actualizar el modelo --> lr * gradiente\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _get_accuracy(self, predicted, actual):\n", "        \"\"\"\n", "        Calcular accuracy despu\u00e9s de cada iteraci\u00f3n\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _calculate_loss(self, predicted, actual):\n", "        \"\"\"\n", "        Calcular cross-entropy loss despu\u00e9s de cada iteraci\u00f3n\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def train(self, X_train, y_train, epochs):\n", "        \"\"\"\n", "        Entrenar el modelo Stochastic Gradient Descent\n", "        \"\"\"\n", "        raise NotImplementedError"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Los items que se presentan a continuaci\u00f3n tienen como objetivo explorar las clases que componen la red neuronal propuesta, comprender su arquitectura y funcionamiento.\n", "\n", "Nuevamente, lo ideal es no mirar todos los m\u00e9todos hasta que llegue el momento de utilizarlos. \n", "\n", "1. Crear una Red Neuronal con 6 nodos en la primera capa, 8 en la segunda, 10 en la tercer y finalmente 3 en la \u00faltima, utilizando los m\u00e9todos `add()`, `_compile()` de la clase `RedNeuronal` y el constructor de la clase `Capa`.\n", "  \n", "    Imprimir la arquitectura del modelo y asegurarse de obtener:\n", "\n", "    ```\n", "    [{'input_dim': 4, 'output_dim': 6, 'activation': 'relu'},\n", "    {'input_dim': 6, 'output_dim': 8, 'activation': 'relu'},\n", "    {'input_dim': 8, 'output_dim': 10, 'activation': 'relu'},\n", "    {'input_dim': 10, 'output_dim': 3, 'activation': 'softmax'}]\n", "    ```\n", "\n", "    Dibujar la red en papel.\n", "\n", "1. Inicializar los pesos de la red del punto anterior (`_init_weights(datos)`) y verificar que los pesos tienen dimensi\u00f3n correcta:\n", "\n", "    ```\n", "    capa 0: w=(4, 6) - b=(1, 6)\n", "    capa 1: w=(6, 8) - b=(1, 8)\n", "    capa 2: w=(8, 10) - b=(1, 10)\n", "    capa 3: w=(10, 3) - b=(1, 3)\n", "    ```\n", "\n", "    Definir las matrices que se corresponden con las capas de manera que una pasada pueda ser interpretada como el producto de todas ellas. Recordar que en cada paso por cada capa estaremos computando por cada neurona de la capa siguiente:\n", "\n", "    $$Z = \\sum_{i=1}^{n} X_i \\times W_i + b$$\n", "\n", "1. Funciones de activaci\u00f3n de una `Capa`:\n", "\n", "    1. Verificar que el funcionamiento de `ReLU` se corresponda con:\n", "\n", "        ```\n", "        if input > 0:\n", "            return input\n", "        else:\n", "            return 0\n", "        ``` \n", "\n", "    1. Verificar que el funcionamiento de `softmax` se corresponda con:\n", "\n", "        $$\\sigma(Z)_i = \\frac{e^{z_i}}{\\sum_{i=1}^{n} e^{z_j}}$$\n", "\n", "    **Nota**: para probar estos dos m\u00e9todos puede ser util construir un vector de la siguiente manera: `np.array([[1.3, 5.1, -2.2, 0.7, 1.1]])` que genera un vector de tama\u00f1o (1,5).\n", "\n", "1. Avancemos con `_forwardprop(datos)`, si corremos la red inicializada con los datos:\n", "\n", "    1. \u00bfQu\u00e9 nos tipo de objeto nos devuelve este m\u00e9todo?\n", "\n", "    1. \u00bfQu\u00e9 quiere decir cada uno de los valores?\n", "\n", "    1. La primera fila, que se corresponder\u00eda con la primera observaci\u00f3n del dataset, \u00bfqu\u00e9 resultados nos da?\u00bfqu\u00e9 es m\u00e1s probable: 'setosa', 'versicolor' o 'virginica'?\u00bfqu\u00e9 valor es el real?\u00bfpor qu\u00e9?\n", "\n", "1. Arrancamos a propagar para atr\u00e1s lo aprendido en la primera pasada. Esto lo realizaremos con el m\u00e9todo `_backprop`.\n", "\n", "    1. \u00bfC\u00f3mo es la derivada de la funci\u00f3n de activaci\u00f3n `ReLU`?\u00bfSu c\u00f3digo es correcto?\n", "\n", "    1. \u00bfCu\u00e1l es la operaci\u00f3n matem\u00e1tica que hace la funci\u00f3n `backward` de la clase `Capa` en el caso de tener como activaci\u00f3n a `relu`?\n", "\n", "    1. El m\u00e9todo `_backprop` toma 2 par\u00e1metros: `predicted` y `actual`. \u00bfqu\u00e9 debemos pasarle en dicho lugar?\n", "\n", "        Si la respuesta no fue: en `predicted` le pasamos el resultado de `_forwardprop(...)` y en `actual` le pasamos `y`.... volver a pensarlo. ;-)\n", "\n", "    1. Verificar que los `gradientes` y los `pesos` para cada una de las capas tienen el mismo tama\u00f1o.\n", "\n", "1. Preparemos por \u00faltimo las funciones necesarias para el entrenamiento. Describir brevemente qu\u00e9 hacen las funciones:\n", "\n", "    - `_get_accuracy`\n", "    - `_calculate_loss`\n", "    - `_update`\n", "\n", "1. Incluyamos finalmente la funci\u00f3n `train` y entrenemos una red con la arquitectura propuesta en el punto 1 por 200 epocas.\n", "\n", "    1. \u00bfQu\u00e9 valores se imprimen?\u00bfQu\u00e9 es posible interpretar de ellos?\n", "\n", "    1. Graficar el _accuracy_ y la _loss_ que arroja el entramiento en funci\u00f3n de las _epochs_. \u00bfQu\u00e9 se puede concluir? Probablemente la se\u00f1al sea ruidosa, por lo que se recomienda hacer un suavizado por ventanas deslizantes.\n", "\n", "\n", "Cr\u00e9dito: este ejercicio se base en la propuesta de Joe Sasson publicada en [Towards Data Science](https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### C\u00f3digo completo\n"]}, {"cell_type": "code", "execution_count": 390, "metadata": {}, "outputs": [], "source": ["class Capa:\n", "    def __init__(self, neuronas):\n", "        self.neuronas = neuronas\n", "\n", "    def forward(self, inputs, weights, bias, activation):\n", "        \"\"\"\n", "        Forward Propagation de la capa\n", "        \"\"\"\n", "        Z_curr = np.dot(inputs, weights.T) + bias\n", "\n", "        if activation == 'relu':\n", "            A_curr = self.relu(inputs=Z_curr)\n", "        elif activation == 'softmax':\n", "            A_curr = self.softmax(inputs=Z_curr)\n", "\n", "        return A_curr, Z_curr\n", "\n", "    def relu(self, inputs):\n", "        \"\"\"\n", "        ReLU: funci\u00f3n de activaci\u00f3n\n", "        \"\"\"\n", "\n", "        return np.maximum(0, inputs)\n", "\n", "    def softmax(self, inputs):\n", "        \"\"\"\n", "        Softmax: funci\u00f3n de activaci\u00f3n\n", "        \"\"\"\n", "        exp_scores = np.exp(inputs)\n", "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n", "        return probs\n", "         \n", "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n", "        \"\"\"\n", "        Backward Propagation de la capa\n", "        \"\"\"\n", "        if activation == 'softmax':\n", "            dW = np.dot(A_prev.T, dA_curr)\n", "            db = np.sum(dA_curr, axis=0, keepdims=True)\n", "            dA = np.dot(dA_curr, W_curr) \n", "        else:\n", "            dZ = self.relu_derivative(dA_curr, Z_curr)\n", "            dW = np.dot(A_prev.T, dZ)\n", "            db = np.sum(dZ, axis=0, keepdims=True)\n", "            dA = np.dot(dZ, W_curr)\n", "            \n", "        return dA, dW, db\n", "\n", "    def relu_derivative(self, dA, Z):\n", "        \"\"\"\n", "        ReLU: gradiente de ReLU\n", "        \"\"\"\n", "        dZ = np.array(dA, copy = True)\n", "        dZ[Z <= 0] = 0\n", "        return dZ\n", "    "]}, {"cell_type": "code", "execution_count": 409, "metadata": {}, "outputs": [], "source": ["class RedNeuronal:\n", "    def __init__(self, learning_rate=0.01):\n", "        self.red = [] ## capas\n", "        self.arquitectura = [] ## mapeo de entradas -> salidas\n", "        self.pesos = [] ## W, b\n", "        self.memoria = [] ## Z, A\n", "        self.gradientes = [] ## dW, db\n", "        self.lr = learning_rate\n", "        \n", "    def add(self, capa):\n", "        \"\"\"\n", "        Agregar capa a la red\n", "        \"\"\"\n", "        self.red.append(capa)\n", "            \n", "    def _compile(self, data):\n", "        \"\"\"\n", "        Inicializar la arquitectura\n", "        \"\"\"\n", "        for idx, _ in enumerate(self.red):\n", "            if idx == 0:\n", "                self.arquitectura.append({'input_dim': data.shape[1], \n", "                                        'output_dim': self.red[idx].neuronas,\n", "                                        'activation':'relu'})\n", "            elif idx > 0 and idx < len(self.red)-1:\n", "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas, \n", "                                        'output_dim': self.red[idx].neuronas,\n", "                                        'activation':'relu'})\n", "            else:\n", "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas, \n", "                                        'output_dim': self.red[idx].neuronas,\n", "                                        'activation':'softmax'})\n", "        return self\n", "\n", "    def _init_weights(self, data):\n", "        \"\"\"\n", "        Inicializar arquitectura y los pesos\n", "        \"\"\"\n", "        self._compile(data)\n", "\n", "        np.random.seed(99)\n", "\n", "        for i in range(len(self.arquitectura)):\n", "            self.pesos.append({\n", "                'W':np.random.uniform(low=-1, high=1, \n", "                        size=(self.arquitectura[i]['input_dim'],\n", "                            self.arquitectura[i]['output_dim']\n", "                            )),\n", "                'b':np.zeros((1, self.arquitectura[i]['output_dim']))})\n", "\n", "        return self\n", "    \n", "    def _forwardprop(self, data):\n", "        \"\"\"\n", "        Pasada forward completa por la red\n", "        \"\"\"\n", "        A_curr = data\n", "\n", "        for i in range(len(self.pesos)):\n", "            A_prev = A_curr\n", "            A_curr, Z_curr = self.red[i].forward(inputs=A_prev, \n", "                                                    weights=self.pesos[i]['W'].T, \n", "                                                    bias=self.pesos[i]['b'], \n", "                                                    activation=self.arquitectura[i]['activation'])\n", "\n", "            self.memoria.append({'inputs':A_prev, 'Z':Z_curr})\n", "\n", "        return A_curr\n", "    \n", "    def _backprop(self, predicted, actual):\n", "        \"\"\"\n", "        Pasada backward completa por la red\n", "        \"\"\"\n", "        num_samples = len(actual)\n", "\n", "        ## compute the gradient on predictions\n", "        dscores = predicted\n", "        dscores[range(num_samples),actual] -= 1\n", "        dscores /= num_samples\n", "\n", "        dA_prev = dscores\n", "\n", "        for idx, layer in reversed(list(enumerate(self.red))):\n", "            dA_curr = dA_prev\n", "\n", "            A_prev = self.memoria[idx]['inputs']\n", "            Z_curr = self.memoria[idx]['Z']\n", "            W_curr = self.pesos[idx]['W']\n", "\n", "            activation = self.arquitectura[idx]['activation']\n", "\n", "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr.T, Z_curr, A_prev, activation)\n", "\n", "            self.gradientes.append({'dW':dW_curr, 'db':db_curr})\n", "\n", "        self.gradientes = list(reversed(self.gradientes))  # Reverse the gradients list\n", "\n", "    def _update(self):\n", "        \"\"\"\n", "        Actualizar el modelo --> lr * gradiente\n", "        \"\"\"\n", "        lr = self.lr\n", "        for idx, layer in enumerate(self.red):\n", "            self.pesos[idx]['W'] -= lr * self.gradientes[idx]['dW']\n", "            self.pesos[idx]['b'] -= lr * self.gradientes[idx]['db']\n", "\n", "    def _get_accuracy(self, predicted, actual):\n", "        \"\"\"\n", "        Calcular accuracy despu\u00e9s de cada iteraci\u00f3n\n", "        \"\"\"\n", "        return np.mean(np.argmax(predicted, axis=1)==actual)\n", "        \n", "    def _calculate_loss(self, predicted, actual):\n", "        \"\"\"\n", "        Calculate cross-entropy loss after each iteration\n", "        \"\"\"\n", "        samples = len(actual)\n", "\n", "        correct_logprobs = -np.log(predicted[range(samples),actual])\n", "        data_loss = np.sum(correct_logprobs)/samples\n", "\n", "        return data_loss\n", "\n", "    def train(self, X_train, y_train, epochs):\n", "        \"\"\"\n", "        Entrenar el modelo Stochastic Gradient Descent\n", "        \"\"\"\n", "        self.loss = []\n", "        self.accuracy = []\n", "\n", "        self._init_weights(X_train)\n", "\n", "        for i in range(epochs):\n", "            yhat = self._forwardprop(X_train)\n", "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n", "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n", "\n", "            self._backprop(predicted=yhat, actual=y_train)\n", "\n", "            self._update()\n", "\n", "            if i % 20 == 0:\n", "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n", "                print(s)\n", "\n", "        return (self.accuracy, self.loss)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from graphviz import Digraph, Graph\n", "\n", "def dibujar_red(red):\n", "    dot = Graph()\n", "    dot.attr(rankdir=\"LR\")\n", "    dot.attr(splines=\"false\")\n", "    dot.attr(nodesep=\"0.05\")\n", "    \n", "    for idx,capa in enumerate(red.arquitectura):\n", "        with dot.subgraph(name=f'cluster_{idx}') as c:\n", "            c.attr(rank=\"same\")\n", "            for i in range(capa['input_dim']+1):\n", "                c.node(nombre_nodo(idx, i), label=etiqueta_nodo(idx,i))\n", "\n", "            c.attr(color='white')\n", "            \n", "            label_extra = \"Entrada\" if idx == 0 else \"Oculta\\n(ReLU)\"\n", "        \n", "            c.attr(label=f'capa {idx+1}\\n{label_extra}')\n", "\n", "    with dot.subgraph(name=f'cluster_{idx+1}') as c:\n", "            c.attr(rank=\"same\")\n", "            for i in range(capa['output_dim']):\n", "                c.node(nombre_nodo(idx+1, i), label=etiqueta_nodo(idx+1,i, True))\n", "\n", "            c.attr(color='white')\n", "            \n", "            label_extra = \"Salida\\n(SoftMax)\"\n", "        \n", "            c.attr(label=f'capa {idx+1}\\n{label_extra}')\n", "\n", "    for idx, capa in enumerate(red.arquitectura):\n", "        for in_idx in range(capa[\"input_dim\"]+1):\n", "            for out_idx in range(capa[\"output_dim\"]):\n", "                to_node = (idx+1, out_idx+1) if idx!=len(red.arquitectura)-1 else (idx+1, out_idx)\n", "                dot.edge(nombre_nodo(idx, in_idx), \n", "                         nombre_nodo(*to_node))\n", "\n", "    return dot\n", "\n", "def nombre_nodo(capa, indice):\n", "    res = f\"c_{capa}_{indice}\"\n", "    return res\n", "\n", "def etiqueta_nodo(capa, indice, es_final=False):\n", "    if indice==0 and not es_final:\n", "        return \"1\"\n", "    l = \"a\" if capa!=0 else \"x\"\n", "    l = l if not es_final else \"y\"\n", "    \n", "    if l==\"x\" or l==\"y\":    \n", "        return f\"<{l}<sub>{indice}</sub>>\"\n", "    else:\n", "        return f\"<{l}<sub>{indice}</sub><sup>({capa})</sup>>\"\n", "\n", "# model = RedNeuronal()\n", "# model.add(...)\n", "# model._compile(...datos...)\n", "\n", "dibujar_red(model)"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}